# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s15KTbVyvHhOfA6yfCoNS8gSFcaFtY9J
"""

import streamlit as st
import numpy as np
import wfdb
import wfdb.processing
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
import os
os.system("pip install wfdb")
import pandas as pd
# Try importing wfdb safely
try:
    import wfdb
    import wfdb.processing
    WFDB_AVAILABLE = True
except ImportError:
    WFDB_AVAILABLE = False
    st.warning("âš ï¸ 'wfdb' library not found. Install it using: pip install wfdb")
# --- 1. Load Model ---
@st.cache_resource
def load_my_model():
    model_path = 'my_summed_signal_model.keras'
    if not os.path.exists(model_path):
        st.error("Model file not found. Please place 'my_summed_signal_model.keras' in the same folder.")
        return None
    try:
        return tf.keras.models.load_model(model_path)
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None

@st.cache_data
def get_label_map():
    return {0: 'F', 1: 'N', 2: 'S', 3: 'V'}

# --- 2. Full Preprocessing Pipeline ---
def run_full_pipeline(record_name, database_name, uploaded_file=None):
    """
    Runs preprocessing for both PhysioNet and uploaded hospital/test ECG data.
    """

    # --- A. Read Data ---
    if uploaded_file is not None:
        st.info("Reading uploaded hospital/test ECG data...")
        try:
            df = pd.read_csv(uploaded_file)
            # --- NEW: Automatically detect column format ---
            if df.shape[1] == 1:
                signal = df.iloc[:, 0].values
            elif 'ECG' in df.columns[0].upper():
                signal = df[df.columns[0]].values
            else:
                signal = df.iloc[:, 0].values  # fallback

            fs = 250  # default sample rate for external test data
            record_name = uploaded_file.name
            st.success(f"âœ… Successfully read uploaded file '{record_name}' (fs={fs}Hz).")

        except Exception as e:
            st.error(f"Error reading uploaded ECG: {e}")
            return None, None
    else:
        st.info(f"Downloading {record_name} from {database_name}...")
        try:
            record = wfdb.rdrecord(record_name, pn_dir=database_name)
            fs = record.fs
            if 'MLII' in record.sig_name:
                mlii_index = record.sig_name.index('MLII')
            elif 'II' in record.sig_name:
                mlii_index = record.sig_name.index('II')
            else:
                mlii_index = 0
            signal = record.p_signal[:, mlii_index]
        except Exception as e:
            st.error(f"Error downloading record: {e}")
            return None, None

    # --- B. Denoise & Normalize ---
    st.info("Filtering & scaling ECG signal...")
    try:
        filtered = wfdb.processing.bandpass_filter(signal, fs=fs, lowcut=0.5, highcut=30)
        scaler = StandardScaler()
        scaled_signal = scaler.fit_transform(filtered.reshape(-1, 1)).flatten()
    except Exception as e:
        st.error(f"Error in filtering/scaling: {e}")
        return None, None

    # --- C. Peak Detection ---
    st.info("Running QRS detection (XQRS)...")
    try:
        xqrs = wfdb.processing.XQRS(sig=scaled_signal, fs=fs)
        xqrs.detect(verbose=False)
        detected_peaks = xqrs.qrs_inds
    except Exception as e:
        st.error(f"QRS detection failed: {e}")
        return None, None

    # --- D. Segmentation ---
    window_size = 180
    half = window_size // 2
    heartbeats = []
    for pk in detected_peaks:
        start = pk - half
        end = pk + half
        if start >= 0 and end < len(scaled_signal):
            heartbeats.append(scaled_signal[start:end])

    X_data = np.array(heartbeats)
    if X_data.shape[0] == 0:
        st.error("No valid heartbeats found in the signal.")
        return None, None

    X_cnn = np.expand_dims(X_data, axis=-1)
    return X_cnn, record_name

# --- 3. Streamlit App ---
st.title("ğŸ«€ ECG Arrhythmia Classification (MIT-BIH Compatible)")
st.write("Upload any ECG signal (hospital/test) or select MIT-BIH record to classify beats.")
st.write("---")

model = load_my_model()
if model is None:
    st.stop()

label_map = get_label_map()

# --- Input Mode ---
mode = st.radio("Select input mode:", ["MIT-BIH Record", "Upload ECG File"])

if mode == "MIT-BIH Record":
    record_option = st.selectbox(
        "Choose PhysioNet record:",
        ("100 (MIT-BIH Database)", "I01 (INCART Database - Cross-Domain Test)")
    )
    if record_option == "100 (MIT-BIH Database)":
        record_name, db_name = '100', 'mitdb'
    else:
        record_name, db_name = 'I01', 'incartdb'
    uploaded_file = None

else:
    uploaded_file = st.file_uploader("Upload hospital/test ECG file (.csv, .dat, .txt)", type=["csv", "dat", "txt"])
    record_name, db_name = None, None

# --- Run Analysis ---
if st.button("Run Analysis", type="primary"):
    with st.spinner("Running analysis pipeline... please wait"):
        X_cnn, used_record = run_full_pipeline(record_name, db_name, uploaded_file)
        if X_cnn is not None:
            st.info("Predicting arrhythmia classes...")
            predictions_prob = model.predict(X_cnn)
            predictions_class = np.argmax(predictions_prob, axis=1)
            predicted_labels = [label_map.get(p, 'Unknown') for p in predictions_class]

            st.success("âœ… Analysis complete!")
            st.write("---")

            # --- Display results ---
            st.header(f"Results for: {used_record}")
            counts_df = pd.Series(predicted_labels).value_counts().reset_index()
            counts_df.columns = ['Beat Type', 'Count']
            st.dataframe(counts_df)
            st.bar_chart(counts_df.set_index('Beat Type'))
            st.write("**First 20 Predictions:**", predicted_labels[:20])
